{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvcOfJjNx2Md"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--yn8mIUqxxN"
      },
      "source": [
        "Here, we are loading both the training and testing datasets for two different seasons (21 and 22) of player data from FIFA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiULUIWHx-TY"
      },
      "outputs": [],
      "source": [
        "# Load the 'players_21' dataset for training and the 'players_22' dataset for testing\n",
        "fifa_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mycopy/players_21.csv')\n",
        "fifa_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mycopy/players_22.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotA5VLgrAbi"
      },
      "source": [
        "Executing this block of code, helps to quickly assess the structure and quality of the training dataset, including the number of columns, data types, and the extent of missing data. This information is fundamental to understanding the data and preparing it for further analysis and machine learning model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ue6-os7x_DU"
      },
      "outputs": [],
      "source": [
        "# Explore the training data(players_21)\n",
        "print(\"Training Data Info:\")\n",
        "print(fifa_train.info())\n",
        "print(\"\\nMissing Values in Training Data:\")\n",
        "print(fifa_train.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U4AS11IyB2L"
      },
      "outputs": [],
      "source": [
        "# Explore the testing data(players_22)\n",
        "print(\"\\nTesting Data Info:\")\n",
        "print(fifa_test.info())\n",
        "print(\"\\nMissing Values in Testing Data:\")\n",
        "print(fifa_test.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4dbpdYwrqMV"
      },
      "source": [
        "these code blocks are used to remove specific columns from the fifa_train and fifa_test datasets that are considered unwanted or irrelevant for the analysis or machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L-aWUwJz4CE"
      },
      "outputs": [],
      "source": [
        "unwanted_columns_21 = ['player_url','player_face_url ', 'club_logo_url', 'club_flag_url','nation_logo_url','nation_flag_url' ]\n",
        "fifa_train_cleaned = fifa_train.drop(columns=unwanted_columns_21, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shCoyMrM27dM"
      },
      "outputs": [],
      "source": [
        "unwanted_columns_22 = ['player_url','player_face_url ', 'club_logo_url', 'club_flag_url','nation_logo_url','nation_flag_url' ]\n",
        "fifa_test_cleaned = fifa_test.drop(columns=unwanted_columns_22, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_train_cleaned"
      ],
      "metadata": {
        "id": "LxXXWhMxbFrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eF8LdiO3JDd"
      },
      "outputs": [],
      "source": [
        "fifa_train_cleaned[[\"release_clause_eur\",\"value_eur\",\"dob\",\"potential\",\"age\",\"gk\",\"sofifa_id\", \"rb\" ]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_train_cleaned[[\"release_clause_eur\",\"value_eur\",\"dob\",\"potential\",\"age\",\"gk\",\"sofifa_id\", \"rb\" ]].info()\n",
        "fifa_train_cleaned[\"movement_reactions\"].info()\n",
        "fifa_train_cleaned[\"wage_eur\" ].info()\n",
        " ##  \" wage_eur\""
      ],
      "metadata": {
        "id": "7_KwMMpqsVKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk2uU1P23a53"
      },
      "outputs": [],
      "source": [
        "fifa_test_cleaned.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5O9tvhXsVJq"
      },
      "source": [
        "By removing columns with a significant number of missing values (in this example, more than 30%), this method deals with missing data. High amounts of missing data are often deleted from columns because they could not include information that is valuable for analysis or modeling, and their inclusion could skew the findings. The datasets utilized for analysis and machine learning are as clear and useful as possible thanks to this data pretreatment phase.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgFAadX9nqHh"
      },
      "outputs": [],
      "source": [
        "# Identify columns with more than 30% missing values in the training dataset\n",
        "missing_threshold_21 = (fifa_train_cleaned.isnull().sum() / len(fifa_train_cleaned)) * 100\n",
        "columns_21_with_high_missing = missing_threshold_21[missing_threshold_21 > 30].index\n",
        "\n",
        "# Drop the identified columns from the training dataset\n",
        "fifa_train_cleaned.drop(columns=columns_21_with_high_missing, inplace=True)\n",
        "\n",
        "# Identify columns with more than 30% missing values in the test dataset\n",
        "missing_threshold_22 = (fifa_test_cleaned.isnull().sum() / len(fifa_test_cleaned)) * 100\n",
        "columns_22_with_high_missing = missing_threshold_22[missing_threshold_22 > 30].index\n",
        "\n",
        "# Drop the identified columns from the test dataset\n",
        "fifa_test_cleaned.drop(columns=columns_22_with_high_missing, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlBwA13essBg"
      },
      "source": [
        "Categorical and non-categorical (usually numerical) columns in the training (fifa_train_cleaned) and test (fifa_test_cleaned) datasets are identified and divided into two groups by this code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHwhcCVwnrbt"
      },
      "outputs": [],
      "source": [
        "# Identify categorical columns by selecting columns with 'object' data type\n",
        "categorical_columns_21 = fifa_train_cleaned.select_dtypes(include=['object']).columns\n",
        "non_categorical_columns_21 = fifa_train_cleaned.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "# For the test data (assuming you want to perform the same separation)\n",
        "categorical_columns_22 = fifa_test_cleaned.select_dtypes(include=['object']).columns\n",
        "non_categorical_columns_22 = fifa_test_cleaned.select_dtypes(exclude=['object']).columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3kNubvgtJRr"
      },
      "source": [
        "This code block demonstrates the use of a SimpleImputer from scikit-learn to handle missing values in numeric columns in both the training (fifa_train_cleaned) and test (fifa_test_cleaned) datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JIdmvwKn_0t"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a SimpleImputer for numeric columns with the 'mean' strategy\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform the imputer on the training dataset (numeric columns)\n",
        "fifa_train_cleaned[non_categorical_columns_21] = numeric_imputer.fit_transform(fifa_train_cleaned[non_categorical_columns_21])\n",
        "\n",
        "# Transform the imputer on the test dataset (numeric columns)\n",
        "fifa_test_cleaned[non_categorical_columns_22] = numeric_imputer.transform(fifa_test_cleaned[non_categorical_columns_22])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWco7Yk5tPpc"
      },
      "source": [
        "This code block demonstrates the use of a SimpleImputer for handling missing values in categorical columns (columns with data type 'object') in both the training (fifa_train_cleaned) and test (fifa_test_cleaned) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9u8fIZaoH3K"
      },
      "outputs": [],
      "source": [
        "# Create a SimpleImputer for categorical columns with the 'most_frequent' strategy\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit and transform the imputer on the training dataset (categorical columns)\n",
        "fifa_train_cleaned[categorical_columns_21] = categorical_imputer.fit_transform(fifa_train_cleaned[categorical_columns_21])\n",
        "\n",
        "# Transform the imputer on the test dataset (categorical columns)\n",
        "fifa_test_cleaned[categorical_columns_22] = categorical_imputer.transform(fifa_test_cleaned[categorical_columns_22])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6VEbjt7vRvN"
      },
      "source": [
        "Encoding categorical columns is essential when working with machine learning algorithms, as they often require numerical input. The LabelEncoder is used here to transform categorical values into numerical labels, making the data suitable for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1ysQ9y0oWOf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Combine the training and test datasets\n",
        "fifa_combined = pd.concat([fifa_train_cleaned, fifa_test_cleaned], axis=0)\n",
        "\n",
        "# Initialize a LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode categorical columns in the combined dataset\n",
        "for column in categorical_columns_21:\n",
        "    fifa_combined[column] = label_encoder.fit_transform(fifa_combined[column])\n",
        "\n",
        "# Split the combined dataset back into training and test datasets\n",
        "fifa_train_encoded = fifa_combined[:len(fifa_train_cleaned)]\n",
        "fifa_test_encoded = fifa_combined[len(fifa_train_cleaned):]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmieKJ0jvdKT"
      },
      "source": [
        " the encoded categorical columns and the original numerical columns are combined to create new datasets for both the training and test datasets.The purpose of combining encoded categorical and numerical columns in this manner is to prepare the data for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWxcPvLwpUEy"
      },
      "outputs": [],
      "source": [
        " #Combine encoded categorical and numerical columns for training dataset\n",
        "fifa_train_combined = pd.concat([fifa_train_encoded[categorical_columns_21], fifa_train_cleaned[non_categorical_columns_21]], axis=1)\n",
        "\n",
        "# Combine encoded categorical and numerical columns for test dataset\n",
        "fifa_test_combined = pd.concat([fifa_test_encoded[categorical_columns_21], fifa_test_cleaned[non_categorical_columns_21]], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09rO8enMvzEF"
      },
      "source": [
        "Random Forest Regressor model is used to calculate feature importances for the training dataset, and the top N most important features are printed.\n",
        "\n",
        "The feature importances may be used for feature selection and interpretation and are useful for determining which features have the most impact on the model's predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_K8OWbMp9pZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Extract the target variable 'overall' from the datasets\n",
        "y_train = fifa_train_combined['overall']\n",
        "y_test = fifa_test_combined['overall']\n",
        "\n",
        "# Split the combined training dataset into features and target variable\n",
        "X_train = fifa_train_combined.drop(columns=['overall'])\n",
        "X_test = fifa_test_combined.drop(columns=['overall'])\n",
        "# Create a RandomForestRegressor model to calculate feature importances\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "sorted_feature_importance = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Select the top N features (e.g., top 10)\n",
        "N = 10\n",
        "top_N_features = sorted_feature_importance['Feature'][:N]\n",
        "\n",
        "# Print the top N features\n",
        "print(top_N_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFlYDh5pqI4f"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Extract the target variable 'overall' from the datasets\n",
        "y_test = fifa_test_combined['overall']\n",
        "\n",
        "# Split the combined test dataset into features and target variable\n",
        "X_test = fifa_test_combined.drop(columns=['overall'])\n",
        "\n",
        "# Create a RandomForestRegressor model to calculate feature importances\n",
        "rf22_model = RandomForestRegressor(random_state=42)\n",
        "rf22_model.fit(X_test, y_test)  # Fit the model with the test set data\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances_22 = rf22_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature names and their importance scores\n",
        "feature_importance_df22 = pd.DataFrame({'Feature': X_test.columns, 'Importance': feature_importances_22})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "sorted_feature_importance_22 = feature_importance_df22.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Select the top N features (e.g., top 10)\n",
        "N = 10\n",
        "top_N_features_22 = sorted_feature_importance_22['Feature'][:N]\n",
        "\n",
        "# Print the top N features\n",
        "print(top_N_features_22)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_N_features_21.info()"
      ],
      "metadata": {
        "id": "A82BNBG9rsHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TCt56U9wLP1"
      },
      "source": [
        "scaling the features is to ensure that all features have the same scale, making it easier for machine learning models to learn from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPz468CzyC65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ffc6c521-d1ed-4f69-fa55-e7319deb5af5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-902382dc9a8c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfifa_train_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_N_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfifa_train_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize the StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fifa_train_combined' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train=fifa_train_combined[top_N_features]\n",
        "Y_train=fifa_train_combined['overall']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# Fit and transform the scaler on the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Now, X_train_scaled and X_test_scaled contain your scaled features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled[top_N_features_22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "pC8Zf_tJzux6",
        "outputId": "31588224-226d-4ae8-88e8-7892c87a8c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-f6829b559aa7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_N_features_22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDWtMH50q7sD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Reshape the target arrays to be 2D\n",
        "y_train_scaled = y_train.values.reshape(-1, 1)\n",
        "y_test_scaled = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Fit and transform the scaler on the training data\n",
        "y_train_scaled = scaler.fit_transform(y_train_scaled)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "y_test_scaled = scaler.transform(y_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E4116YrweYI"
      },
      "source": [
        "For developing and testing machine learning models, this stage is essential. It makes sure the model is tested on a different subset of data than it was trained on, allowing you to evaluate the model's performance and generalization skills.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeEE9K2Ry2aY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the scaled data into training and testing sets\n",
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQs78IzNwmjJ"
      },
      "source": [
        "Random Forest algorithm is used to build a regression model for predicting the target variable. It also employs hyperparameter tuning using GridSearchCV.\n",
        "\n",
        "This method effectively uses grid search across a variety of hyperparameters to optimize a Random Forest regression model, reporting the performance of the best model on test data. The objective is to identify the hyperparameter configurations that produce the highest prediction performance for the specified dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3f7l3OA7yp2",
        "outputId": "fe31420c-62e3-492d-a850-2c7535ef8988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor MAE with selected features: 0.6301926767002248\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# ... (previous code for data preprocessing, data loading, and scaling)\n",
        "\n",
        "# Feature selection using RandomForest feature importances\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "# Train the model using only the top N features\n",
        "rf_model.fit(X_train[top_N_features], y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [4, 6]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = grid.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_rf_model.predict(X_test_split)\n",
        "mae = mean_absolute_error(y_test_split, y_pred)\n",
        "\n",
        "print(f'RandomForestRegressor MAE with selected features: {mae}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07U3DJeUxAlz"
      },
      "source": [
        "XGBoost regression model is used to build a model for predicting the target variable, and hyperparameter tuning is performed using GridSearchCV.\n",
        "\n",
        "This code is responsible for fine-tuning an XGBoost regression model using grid search over a range of hyperparameters and reporting the performance of the best model on the test data. The goal is to find the hyperparameter settings that lead to the best predictive accuracy for the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1k99YmW9xaD",
        "outputId": "0ea4bb33-747e-4d18-afc2-5e7205d1a672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost MAE with selected features: 0.34472861432185997\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ... (previous code for data preprocessing, data loading, and scaling)\n",
        "\n",
        "# Feature selection using XGBoost feature importances\n",
        "xgb_model = XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Re-evaluate the model with selected features\n",
        "# Define the model\n",
        "xgb_model = XGBRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [4, 6]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_xgb_model.predict(X_test_split)\n",
        "mae = mean_absolute_error(y_test_split, y_pred)\n",
        "\n",
        "print(f'XGBoost MAE with selected features: {mae}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcnmMr3cxXeD"
      },
      "source": [
        "Similar to the previous code for XGBoost, this code is responsible for fine-tuning a Gradient Boosting regression model using grid search over a range of hyperparameters and reporting the performance of the best model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqjTHt2NAjGE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ... (previous code for data preprocessing, data loading, and scaling)\n",
        "\n",
        "# Feature selection using Gradient Boosting feature importances\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Re-evaluate the model with selected features\n",
        "# Define the model\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [2, 4]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Get the best model\n",
        "best_gb_model = grid.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_gb_model.predict(X_test_split)\n",
        "mae = mean_absolute_error(y_test_split, y_pred)\n",
        "\n",
        "print(f'Gradient Boosting MAE with selected features: {mae}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wCd9-PTxff_"
      },
      "source": [
        "an ensemble model called VotingRegressor is created by combining three individual regression models (XGBoost, Random Forest, and Gradient Boosting) to make predictions\n",
        "\n",
        "Ensemble models combine the strengths of multiple individual models to potentially improve predictive accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILDJC7OJA01l",
        "outputId": "f25da32a-7b2d-4d3a-e942-56394e106090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model MAE with selected features: 0.3983922815322819\n"
          ]
        }
      ],
      "source": [
        "# Create a VotingRegressor ensemble model\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "ensemble_model = VotingRegressor(estimators=[\n",
        "    ('xgb', best_xgb_model),\n",
        "    ('rf', best_rf_model),\n",
        "    ('gb', best_gb_model)\n",
        "])\n",
        "\n",
        "# Fit the ensemble model to the training data\n",
        "ensemble_model.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Evaluate the ensemble model on the test set\n",
        "y_pred = ensemble_model.predict(X_test_split)\n",
        "mae = mean_absolute_error(y_test_split, y_pred)\n",
        "\n",
        "print(f'Ensemble Model MAE with selected features: {mae}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTADI_pxwsU"
      },
      "source": [
        " this code is to determine which of the three models is the most effective in making predictions for the given task. The model with the lowest MAE is considered the best model in terms of predictive accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl1jQjGdCw2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dce46e-4f0d-4aea-893a-5b459e5239e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: XGBRegressor\n",
            "Best Model MAE: 0.34472861432185997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Compare the MAE of the three models\n",
        "mae_rf = mean_absolute_error(y_test_split, best_rf_model.predict(X_test_split))\n",
        "mae_xgb = mean_absolute_error(y_test_split, best_xgb_model.predict(X_test_split))\n",
        "mae_gb = mean_absolute_error(y_test_split, best_gb_model.predict(X_test_split))\n",
        "\n",
        "# Find the model with the lowest MAE\n",
        "best_model = None\n",
        "best_mae = float('inf')\n",
        "\n",
        "if mae_rf < best_mae:\n",
        "    best_model = best_rf_model\n",
        "    best_mae = mae_rf\n",
        "\n",
        "if mae_xgb < best_mae:\n",
        "    best_model = best_xgb_model\n",
        "    best_mae = mae_xgb\n",
        "\n",
        "if mae_gb < best_mae:\n",
        "    best_model = best_gb_model\n",
        "    best_mae = mae_gb\n",
        "\n",
        "# Print the best model and its MAE\n",
        "print(f'Best Model: {best_model.__class__.__name__}')\n",
        "print(f'Best Model MAE: {best_mae}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANClcjgyIZJ"
      },
      "source": [
        "Measuring the Mean Absolute Error (MAE) of the Random Forest model (best_rf_model) on the 'players_22' dataset and reporting it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OyGL113ikux",
        "outputId": "bff6125a-53ca-4d0c-80d5-09adda013a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MAE on players_22: 0.88569326516506\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "# Evaluate the Random Forest model on 'players_22' data\n",
        "y_pred_rf = best_rf_model.predict(X_test_scaled)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(f'Random Forest MAE on players_22: {mae_rf}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDmgQmiDyTyQ"
      },
      "source": [
        "evaluating the XGBoost model (best_xgb_model) on the 'players_22'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkpH3EwdtClD",
        "outputId": "49ff670e-718a-4da3-c763-680bf501c715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost MAE on players_22: 0.5002708766185182\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the XGBoost model on 'players_22' data\n",
        "y_pred_xgb = best_xgb_model.predict(X_test_scaled)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f'XGBoost MAE on players_22: {mae_xgb}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzjpMz71ygNw"
      },
      "source": [
        "evaluating the Gradient Boosting model (best_gb_model) on the 'players_22'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPURESy3tLhm",
        "outputId": "9ed08a2e-98ba-428c-92b9-91cddf3cf952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting MAE on players_22: 1.4691249166812936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Gradient Boosting model on 'players_22' data\n",
        "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "print(f'Gradient Boosting MAE on players_22: {mae_gb}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9AgeMuQy2pT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrrzYFbqy2u3"
      },
      "source": [
        "evaluating the ensemble model (ensemble_model) on the 'players_22' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8qd-6LCtOrq",
        "outputId": "3fc09e9a-f1fa-4007-ea87-658c3a84a223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model MAE on players_22: 0.5879914456843811\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Ensemble model on 'players_22' data\n",
        "y_pred_ensemble = ensemble_model.predict(X_test_scaled)\n",
        "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
        "print(f'Ensemble Model MAE on players_22: {mae_ensemble}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TajC40zjy-vN"
      },
      "source": [
        "comparing the Mean Absolute Error (MAE) values of different models (Random Forest, XGBoost, and Gradient Boosting) to determine which model performed the best on the 'players_22' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Z3PAuDJ7FM",
        "outputId": "f4e267e2-9d5e-4119-afbb-7a14dce41497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best model for \"players_22\" data is: XGBoost with MAE: 0.5002708766185182\n"
          ]
        }
      ],
      "source": [
        "# Compare MAE values to select the best model\n",
        "best_model = None\n",
        "best_mae = float('inf')\n",
        "\n",
        "if mae_rf < best_mae:\n",
        "    best_model = \"Random Forest\"\n",
        "    best_mae = mae_rf\n",
        "\n",
        "if mae_xgb < best_mae:\n",
        "    best_model = \"XGBoost\"\n",
        "    best_mae = mae_xgb\n",
        "\n",
        "if mae_gb < best_mae:\n",
        "    best_model = \"Gradient Boosting\"\n",
        "    best_mae = mae_gb\n",
        "\n",
        "print(f'The best model for \"players_22\" data is: {best_model} with MAE: {best_mae}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Replace 'best_rf_model' with your selected best model (Random Forest, XGBoost, or Gradient Boosting)\n",
        "best_model = best_rf_model\n",
        "\n",
        "# Define the filename for the pickle file\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/mycopy/model.pkl'\n",
        "\n",
        "# Save the best model to a pickle file\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n",
        "\n",
        "print(f\"Best model saved to {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHecKCc5PSQs",
        "outputId": "1b12c1b0-ee28-49df-aae5-dc80f1aa5202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved to /content/drive/My Drive/Colab Notebooks/mycopy/model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyiZHZU6-lRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}